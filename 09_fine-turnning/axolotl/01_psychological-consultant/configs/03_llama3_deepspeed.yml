# model params
# base_model: /home/paul/.cache/huggingface/models/unsloth--Meta-Llama-3.1-8B
# base_model: /home/paul/.cache/huggingface/models/models--Qwen--Qwen2.5-1.5B-Instruct
# base_model: /home/paul/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B
base_model: /root/workspaces/huggingface/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B
# model_type: LlamaForCausalLM
# tokenizer_type: PreTrainedTokenizerFast

# dataset params
datasets:
  - path: data/train.json
    type: chat_template
output_dir: ./models/Llama3_Storyteller_deepspeed

# model params
sequence_length: 1024
pad_to_sequence_len: true
special_tokens:
  pad_token: <|end_of_text|>

bf16: auto
bf32: false

# training params
# batch_size: 16
micro_batch_size: 4
num_epochs: 4
optimize: adamw_bnb_8bit
learning_rate: 0.001

# Gradient Accumulation
gradient_accumulation_steps: 1

# Gradient Checkpointing
gradient_checkpointing: true

logging_steps: 1

# LoRA
adapter: qlora

lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - v_proj
#  - k_proj
#  - o_proj
  - gate_proj
#  - down_proj
#  - up_proj
lora_modules_to_save:
  - embed_tokens
  - lm_head

# lora_target_linear: true


# Low Precision
load_in_8bit: false
load_in_4bit: true

train_on_inputs: false

val_set_size: 0.1
eval_strategy: steps # Set to `"no"` to skip evaluation, `"epoch"` at end of each epoch, leave empty to infer from `eval_steps`.
eval_steps: 50 # Leave empty to eval at each epoch, integer for every N steps. float for fraction of total steps
evals_per_epoch: # number of times per epoch to run evals, mutually exclusive with eval_steps
save_strategy: steps # Set to `"no"` to skip checkpoint saves, `"epoch"` at end of each epoch, `"best"` when better result is achieved, leave empty to infer from `save_steps`.
save_steps: 50 # Leave empty to save at each epoch, integer for every N steps. float for fraction of total steps
saves_per_epoch: # number of times per epoch to save a checkpoint, mutually exclusive with save_steps
save_total_limit: 

special_tokens:
  eos_token: "<|im_end|>"

# prompt_template: |
#   {{ instruction }}{% if input %}\n{{ input }}{% endif %}
#   Let's think step by step: {{ think }}
#   Final Answer: {{ output }}